{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "[[ 0.00966449]\n",
      " [ 0.00786506]\n",
      " [ 0.99358898]\n",
      " [ 0.99211957]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sigmoid function\n",
    "def nonlin(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "    \n",
    "# input dataset\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "    \n",
    "# output dataset            \n",
    "y = np.array([[0,0,1,1]]).T\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "syn0 = 2*np.random.random((3,1)) - 1\n",
    "\n",
    "for iter in xrange(10000):\n",
    "\n",
    "    # forward propagation\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "\n",
    "    # how much did we miss?\n",
    "    l1_error = y - l1\n",
    "\n",
    "    # multiply how much we missed by the \n",
    "    # slope of the sigmoid at the values in l1\n",
    "    l1_delta = l1_error * nonlin(l1,True)\n",
    "\n",
    "    # update weights\n",
    "    syn0 += np.dot(l0.T,l1_delta)\n",
    "\n",
    "print \"Output After Training:\"\n",
    "print l1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.496410031903\n",
      "Error:0.00858452565325\n",
      "Error:0.00578945986251\n",
      "Error:0.00462917677677\n",
      "Error:0.00395876528027\n",
      "Error:0.00351012256786\n",
      "Output After Training:\n",
      "[[ 0.00260572]\n",
      " [ 0.99672209]\n",
      " [ 0.99701711]\n",
      " [ 0.00386759]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nonlin(x,deriv=False):\n",
    "\tif(deriv==True):\n",
    "\t    return x*(1-x)\n",
    "\n",
    "\treturn 1/(1+np.exp(-x))\n",
    "    \n",
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])\n",
    "                \n",
    "y = np.array([[0],\n",
    "\t\t\t[1],\n",
    "\t\t\t[1],\n",
    "\t\t\t[0]])\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# randomly initialize our weights with mean 0\n",
    "syn0 = 2*np.random.random((3,4)) - 1\n",
    "syn1 = 2*np.random.random((4,1)) - 1\n",
    "\n",
    "for j in xrange(60000):\n",
    "\n",
    "\t# Feed forward through layers 0, 1, and 2\n",
    "    l0 = X\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "    l2 = nonlin(np.dot(l1,syn1))\n",
    "\n",
    "    # how much did we miss the target value?\n",
    "    l2_error = y - l2\n",
    "    \n",
    "    if (j% 10000) == 0:\n",
    "        print \"Error:\" + str(np.mean(np.abs(l2_error)))\n",
    "        \n",
    "    # in what direction is the target value?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l2_delta = l2_error*nonlin(l2,deriv=True)\n",
    "\n",
    "    # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "    l1_error = l2_delta.dot(syn1.T)\n",
    "    \n",
    "    # in what direction is the target l1?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    l1_delta = l1_error * nonlin(l1,deriv=True)\n",
    "\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += l0.T.dot(l1_delta)\n",
    "    \n",
    "print \"Output After Training:\"\n",
    "print l2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning the Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training With Alpha:0.001\n",
      "Error after 0 iterations:0.496410031903\n",
      "Error after 10000 iterations:0.495164025493\n",
      "Error after 20000 iterations:0.493596043188\n",
      "Error after 30000 iterations:0.491606358559\n",
      "Error after 40000 iterations:0.489100166544\n",
      "Error after 50000 iterations:0.485977857846\n",
      "\n",
      "Training With Alpha:0.01\n",
      "Error after 0 iterations:0.496410031903\n",
      "Error after 10000 iterations:0.457431074442\n",
      "Error after 20000 iterations:0.359097202563\n",
      "Error after 30000 iterations:0.239358137159\n",
      "Error after 40000 iterations:0.143070659013\n",
      "Error after 50000 iterations:0.0985964298089\n",
      "\n",
      "Training With Alpha:0.1\n",
      "Error after 0 iterations:0.496410031903\n",
      "Error after 10000 iterations:0.0428880170001\n",
      "Error after 20000 iterations:0.0240989942285\n",
      "Error after 30000 iterations:0.0181106521468\n",
      "Error after 40000 iterations:0.0149876162722\n",
      "Error after 50000 iterations:0.0130144905381\n",
      "\n",
      "Training With Alpha:1\n",
      "Error after 0 iterations:0.496410031903\n",
      "Error after 10000 iterations:0.00858452565325\n",
      "Error after 20000 iterations:0.00578945986251\n",
      "Error after 30000 iterations:0.00462917677677\n",
      "Error after 40000 iterations:0.00395876528027\n",
      "Error after 50000 iterations:0.00351012256786\n",
      "\n",
      "Training With Alpha:10\n",
      "Error after 0 iterations:0.496410031903\n",
      "Error after 10000 iterations:0.00312938876301\n",
      "Error after 20000 iterations:0.00214459557985\n",
      "Error after 30000 iterations:0.00172397549956\n",
      "Error after 40000 iterations:0.00147821451229\n",
      "Error after 50000 iterations:0.00131274062834\n",
      "\n",
      "Training With Alpha:100\n",
      "Error after 0 iterations:0.496410031903\n",
      "Error after 10000 iterations:0.125476983834\n",
      "Error after 20000 iterations:0.12533033354\n",
      "Error after 30000 iterations:0.125267728794\n",
      "Error after 40000 iterations:0.1252310737\n",
      "Error after 50000 iterations:0.125206352804\n",
      "\n",
      "Training With Alpha:1000\n",
      "Error after 0 iterations:0.496410031903\n",
      "Error after 10000 iterations:0.5\n",
      "Error after 20000 iterations:0.5\n",
      "Error after 30000 iterations:0.5\n",
      "Error after 40000 iterations:0.5\n",
      "Error after 50000 iterations:0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "alphas = [0.001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "    \n",
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])\n",
    "                \n",
    "y = np.array([[0],\n",
    "\t\t\t[1],\n",
    "\t\t\t[1],\n",
    "\t\t\t[0]])\n",
    "\n",
    "for alpha in alphas:\n",
    "    print \"\\nTraining With Alpha:\" + str(alpha)\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # randomly initialize our weights with mean 0\n",
    "    synapse_0 = 2*np.random.random((3,4)) - 1\n",
    "    synapse_1 = 2*np.random.random((4,1)) - 1\n",
    "\n",
    "    for j in xrange(60000):\n",
    "\n",
    "        # Feed forward through layers 0, 1, and 2\n",
    "        layer_0 = X\n",
    "        layer_1 = sigmoid(np.dot(layer_0,synapse_0))\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        # how much did we miss the target value?\n",
    "        layer_2_error = layer_2 - y\n",
    "\n",
    "        if (j% 10000) == 0:\n",
    "            print \"Error after \"+str(j)+\" iterations:\" + str(np.mean(np.abs(layer_2_error)))\n",
    "\n",
    "        # in what direction is the target value?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_2_delta = layer_2_error*sigmoid_output_to_derivative(layer_2)\n",
    "\n",
    "        # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
    "\n",
    "        # in what direction is the target l1?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
    "\n",
    "        synapse_1 -= alpha * (layer_1.T.dot(layer_2_delta))\n",
    "        synapse_0 -= alpha * (layer_0.T.dot(layer_1_delta))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training With Alpha:0.001\n",
      "Error:0.496410031903\n",
      "Error:0.495164025493\n",
      "Error:0.493596043188\n",
      "Error:0.491606358559\n",
      "Error:0.489100166544\n",
      "Error:0.485977857846\n",
      "Synapse 0\n",
      "[[-0.28448441  0.32471214 -1.53496167 -0.47594822]\n",
      " [-0.7550616  -1.04593014 -1.45446052 -0.32606771]\n",
      " [-0.2594825  -0.13487028 -0.29722666  0.40028038]]\n",
      "Synapse 0 Update Direction Changes\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 1.  0.  1.  1.]]\n",
      "Synapse 1\n",
      "[[-0.61957526]\n",
      " [ 0.76414675]\n",
      " [-1.49797046]\n",
      " [ 0.40734574]]\n",
      "Synapse 1 Update Direction Changes\n",
      "[[ 1.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]]\n",
      "\n",
      "Training With Alpha:0.01\n",
      "Error:0.496410031903\n",
      "Error:0.457431074442\n",
      "Error:0.359097202563\n",
      "Error:0.239358137159\n",
      "Error:0.143070659013\n",
      "Error:0.0985964298089\n",
      "Synapse 0\n",
      "[[ 2.39225985  2.56885428 -5.38289334 -3.29231397]\n",
      " [-0.35379718 -4.6509363  -5.67005693 -1.74287864]\n",
      " [-0.15431323 -1.17147894  1.97979367  3.44633281]]\n",
      "Synapse 0 Update Direction Changes\n",
      "[[ 1.  1.  0.  0.]\n",
      " [ 2.  0.  0.  2.]\n",
      " [ 4.  2.  1.  1.]]\n",
      "Synapse 1\n",
      "[[-3.70045078]\n",
      " [ 4.57578637]\n",
      " [-7.63362462]\n",
      " [ 4.73787613]]\n",
      "Synapse 1 Update Direction Changes\n",
      "[[ 2.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]]\n",
      "\n",
      "Training With Alpha:0.1\n",
      "Error:0.496410031903\n",
      "Error:0.0428880170001\n",
      "Error:0.0240989942285\n",
      "Error:0.0181106521468\n",
      "Error:0.0149876162722\n",
      "Error:0.0130144905381\n",
      "Synapse 0\n",
      "[[ 3.88035459  3.6391263  -5.99509098 -3.8224267 ]\n",
      " [-1.72462557 -5.41496387 -6.30737281 -3.03987763]\n",
      " [ 0.45953952 -1.77301389  2.37235987  5.04309824]]\n",
      "Synapse 0 Update Direction Changes\n",
      "[[ 1.  1.  0.  0.]\n",
      " [ 2.  0.  0.  2.]\n",
      " [ 4.  2.  1.  1.]]\n",
      "Synapse 1\n",
      "[[-5.72386389]\n",
      " [ 6.15041318]\n",
      " [-9.40272079]\n",
      " [ 6.61461026]]\n",
      "Synapse 1 Update Direction Changes\n",
      "[[ 2.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]]\n",
      "\n",
      "Training With Alpha:1\n",
      "Error:0.496410031903\n",
      "Error:0.00858452565325\n",
      "Error:0.00578945986251\n",
      "Error:0.00462917677677\n",
      "Error:0.00395876528027\n",
      "Error:0.00351012256786\n",
      "Synapse 0\n",
      "[[ 4.6013571   4.17197193 -6.30956245 -4.19745118]\n",
      " [-2.58413484 -5.81447929 -6.60793435 -3.68396123]\n",
      " [ 0.97538679 -2.02685775  2.52949751  5.84371739]]\n",
      "Synapse 0 Update Direction Changes\n",
      "[[ 1.  1.  0.  0.]\n",
      " [ 2.  0.  0.  2.]\n",
      " [ 4.  2.  1.  1.]]\n",
      "Synapse 1\n",
      "[[ -6.96765763]\n",
      " [  7.14101949]\n",
      " [-10.31917382]\n",
      " [  7.86128405]]\n",
      "Synapse 1 Update Direction Changes\n",
      "[[ 2.]\n",
      " [ 1.]\n",
      " [ 0.]\n",
      " [ 1.]]\n",
      "\n",
      "Training With Alpha:10\n",
      "Error:0.496410031903\n",
      "Error:0.00312938876301\n",
      "Error:0.00214459557985\n",
      "Error:0.00172397549956\n",
      "Error:0.00147821451229\n",
      "Error:0.00131274062834\n",
      "Synapse 0\n",
      "[[ 4.52597806  5.77663165 -7.34266481 -5.29379829]\n",
      " [ 1.66715206 -7.16447274 -7.99779235 -1.81881849]\n",
      " [-4.27032921 -3.35838279  3.44594007  4.88852208]]\n",
      "Synapse 0 Update Direction Changes\n",
      "[[  7.  19.   2.   6.]\n",
      " [  7.   2.   0.  22.]\n",
      " [ 19.  26.   9.  17.]]\n",
      "Synapse 1\n",
      "[[ -8.58485788]\n",
      " [ 10.1786297 ]\n",
      " [-14.87601886]\n",
      " [  7.57026121]]\n",
      "Synapse 1 Update Direction Changes\n",
      "[[ 22.]\n",
      " [ 15.]\n",
      " [  4.]\n",
      " [ 15.]]\n",
      "\n",
      "Training With Alpha:100\n",
      "Error:0.496410031903\n",
      "Error:0.125476983834\n",
      "Error:0.12533033354\n",
      "Error:0.125267728794\n",
      "Error:0.1252310737\n",
      "Error:0.125206352804\n",
      "Synapse 0\n",
      "[[-17.20515383   1.89881344 -16.95533169  -8.23482697]\n",
      " [  5.7023927  -17.23785048  -9.48052434  -7.92972576]\n",
      " [ -4.18780303  -0.33881828   2.8202323   -8.40059859]]\n",
      "Synapse 0 Update Direction Changes\n",
      "[[  8.   7.   3.   2.]\n",
      " [ 13.   8.   2.   4.]\n",
      " [ 16.  13.  12.   8.]]\n",
      "Synapse 1\n",
      "[[  9.68285305]\n",
      " [  9.5573212 ]\n",
      " [-16.03906837]\n",
      " [  6.27326973]]\n",
      "Synapse 1 Update Direction Changes\n",
      "[[ 13.]\n",
      " [ 11.]\n",
      " [ 12.]\n",
      " [ 10.]]\n",
      "\n",
      "Training With Alpha:1000\n",
      "Error:0.496410031903\n",
      "Error:0.5\n",
      "Error:0.5\n",
      "Error:0.5\n",
      "Error:0.5\n",
      "Error:0.5\n",
      "Synapse 0\n",
      "[[-56.06177241  -4.66916407  -5.65196178 -23.05868769]\n",
      " [ -4.52271708  -4.78629811 -10.887702   -15.85879101]\n",
      " [-89.56678495  10.61497652  37.02351519 -48.33299795]]\n",
      "Synapse 0 Update Direction Changes\n",
      "[[ 3.  2.  4.  1.]\n",
      " [ 1.  2.  2.  1.]\n",
      " [ 6.  6.  4.  1.]]\n",
      "Synapse 1\n",
      "[[  25.16188889]\n",
      " [  -8.65053733]\n",
      " [-104.60697286]\n",
      " [  11.41582458]]\n",
      "Synapse 1 Update Direction Changes\n",
      "[[ 7.]\n",
      " [ 7.]\n",
      " [ 7.]\n",
      " [ 3.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "alphas = [0.001,0.01,0.1,1,10,100,1000]\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "    \n",
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])\n",
    "                \n",
    "y = np.array([[0],\n",
    "\t\t\t[1],\n",
    "\t\t\t[1],\n",
    "\t\t\t[0]])\n",
    "\n",
    "\n",
    "\n",
    "for alpha in alphas:\n",
    "    print \"\\nTraining With Alpha:\" + str(alpha)\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # randomly initialize our weights with mean 0\n",
    "    synapse_0 = 2*np.random.random((3,4)) - 1\n",
    "    synapse_1 = 2*np.random.random((4,1)) - 1\n",
    "        \n",
    "    prev_synapse_0_weight_update = np.zeros_like(synapse_0)\n",
    "    prev_synapse_1_weight_update = np.zeros_like(synapse_1)\n",
    "\n",
    "    synapse_0_direction_count = np.zeros_like(synapse_0)\n",
    "    synapse_1_direction_count = np.zeros_like(synapse_1)\n",
    "        \n",
    "    for j in xrange(60000):\n",
    "\n",
    "        # Feed forward through layers 0, 1, and 2\n",
    "        layer_0 = X\n",
    "        layer_1 = sigmoid(np.dot(layer_0,synapse_0))\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        # how much did we miss the target value?\n",
    "        layer_2_error = y - layer_2\n",
    "\n",
    "        if (j% 10000) == 0:\n",
    "            print \"Error:\" + str(np.mean(np.abs(layer_2_error)))\n",
    "\n",
    "        # in what direction is the target value?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_2_delta = layer_2_error*sigmoid_output_to_derivative(layer_2)\n",
    "\n",
    "        # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
    "\n",
    "        # in what direction is the target l1?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
    "        \n",
    "        synapse_1_weight_update = (layer_1.T.dot(layer_2_delta))\n",
    "        synapse_0_weight_update = (layer_0.T.dot(layer_1_delta))\n",
    "        \n",
    "        if(j > 0):\n",
    "            synapse_0_direction_count += np.abs(((synapse_0_weight_update > 0)+0) - ((prev_synapse_0_weight_update > 0) + 0))\n",
    "            synapse_1_direction_count += np.abs(((synapse_1_weight_update > 0)+0) - ((prev_synapse_1_weight_update > 0) + 0))        \n",
    "        \n",
    "        synapse_1 += alpha * synapse_1_weight_update\n",
    "        synapse_0 += alpha * synapse_0_weight_update\n",
    "        \n",
    "        prev_synapse_0_weight_update = synapse_0_weight_update\n",
    "        prev_synapse_1_weight_update = synapse_1_weight_update\n",
    "    \n",
    "    print \"Synapse 0\"\n",
    "    print synapse_0\n",
    "    \n",
    "    print \"Synapse 0 Update Direction Changes\"\n",
    "    print synapse_0_direction_count\n",
    "    \n",
    "    print \"Synapse 1\"\n",
    "    print synapse_1\n",
    "\n",
    "    print \"Synapse 1 Update Direction Changes\"\n",
    "    print synapse_1_direction_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training With Alpha:0.001\n",
      "Error after 0 iterations:0.496439922501\n",
      "Error after 10000 iterations:0.491049468129\n",
      "Error after 20000 iterations:0.484976307027\n",
      "Error after 30000 iterations:0.477830678793\n",
      "Error after 40000 iterations:0.46903846539\n",
      "Error after 50000 iterations:0.458029258565\n",
      "\n",
      "Training With Alpha:0.01\n",
      "Error after 0 iterations:0.496439922501\n",
      "Error after 10000 iterations:0.356379061648\n",
      "Error after 20000 iterations:0.146939845465\n",
      "Error after 30000 iterations:0.0880156127416\n",
      "Error after 40000 iterations:0.065147819275\n",
      "Error after 50000 iterations:0.0529658087026\n",
      "\n",
      "Training With Alpha:0.1\n",
      "Error after 0 iterations:0.496439922501\n",
      "Error after 10000 iterations:0.0305404908386\n",
      "Error after 20000 iterations:0.0190638725334\n",
      "Error after 30000 iterations:0.0147643907296\n",
      "Error after 40000 iterations:0.0123892429905\n",
      "Error after 50000 iterations:0.0108421669738\n",
      "\n",
      "Training With Alpha:1\n",
      "Error after 0 iterations:0.496439922501\n",
      "Error after 10000 iterations:0.00736052234249\n",
      "Error after 20000 iterations:0.00497251705039\n",
      "Error after 30000 iterations:0.00396863978159\n",
      "Error after 40000 iterations:0.00338641021983\n",
      "Error after 50000 iterations:0.00299625684932\n",
      "\n",
      "Training With Alpha:10\n",
      "Error after 0 iterations:0.496439922501\n",
      "Error after 10000 iterations:0.00225627779119\n",
      "Error after 20000 iterations:0.00153822414303\n",
      "Error after 30000 iterations:0.00123497929147\n",
      "Error after 40000 iterations:0.00105841214497\n",
      "Error after 50000 iterations:0.00093971881704\n",
      "\n",
      "Training With Alpha:100\n",
      "Error after 0 iterations:0.496439922501\n",
      "Error after 10000 iterations:0.5\n",
      "Error after 20000 iterations:0.5\n",
      "Error after 30000 iterations:0.5\n",
      "Error after 40000 iterations:0.5\n",
      "Error after 50000 iterations:0.5\n",
      "\n",
      "Training With Alpha:1000\n",
      "Error after 0 iterations:0.496439922501\n",
      "Error after 10000 iterations:0.5\n",
      "Error after 20000 iterations:0.5\n",
      "Error after 30000 iterations:0.5\n",
      "Error after 40000 iterations:0.5\n",
      "Error after 50000 iterations:0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "alphas = [0.001,0.01,0.1,1,10,100,1000]\n",
    "hiddenSize = 32\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "    \n",
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1]])\n",
    "                \n",
    "y = np.array([[0],\n",
    "\t\t\t[1],\n",
    "\t\t\t[1],\n",
    "\t\t\t[0]])\n",
    "\n",
    "for alpha in alphas:\n",
    "    print \"\\nTraining With Alpha:\" + str(alpha)\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # randomly initialize our weights with mean 0\n",
    "    synapse_0 = 2*np.random.random((3,hiddenSize)) - 1\n",
    "    synapse_1 = 2*np.random.random((hiddenSize,1)) - 1\n",
    "\n",
    "    for j in xrange(60000):\n",
    "\n",
    "        # Feed forward through layers 0, 1, and 2\n",
    "        layer_0 = X\n",
    "        layer_1 = sigmoid(np.dot(layer_0,synapse_0))\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        # how much did we miss the target value?\n",
    "        layer_2_error = layer_2 - y\n",
    "\n",
    "        if (j% 10000) == 0:\n",
    "            print \"Error after \"+str(j)+\" iterations:\" + str(np.mean(np.abs(layer_2_error)))\n",
    "\n",
    "        # in what direction is the target value?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_2_delta = layer_2_error*sigmoid_output_to_derivative(layer_2)\n",
    "\n",
    "        # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
    "\n",
    "        # in what direction is the target l1?\n",
    "        # were we really sure? if so, don't change too much.\n",
    "        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n",
    "\n",
    "        synapse_1 -= alpha * (layer_1.T.dot(layer_2_delta))\n",
    "        synapse_0 -= alpha * (layer_0.T.dot(layer_1_delta))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "[[ 0.14780985]\n",
      " [ 0.97826134]\n",
      " [ 0.87622889]\n",
      " [ 0.15216415]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n",
    "y = np.array([[0,1,1,0]]).T\n",
    "alpha,hidden_dim,dropout_percent,do_dropout = (0.5,4,0.2,True)\n",
    "synapse_0 = 2*np.random.random((3,hidden_dim)) - 1\n",
    "synapse_1 = 2*np.random.random((hidden_dim,1)) - 1\n",
    "for j in xrange(60000):\n",
    "    layer_1 = (1/(1+np.exp(-(np.dot(X,synapse_0)))))\n",
    "    if(do_dropout):\n",
    "        layer_1 *= np.random.binomial([np.ones((len(X),hidden_dim))],1-dropout_percent)[0] * (1.0/(1-dropout_percent))\n",
    "    layer_2 = 1/(1+np.exp(-(np.dot(layer_1,synapse_1))))\n",
    "    layer_2_delta = (layer_2 - y)*(layer_2*(1-layer_2))\n",
    "    layer_1_delta = layer_2_delta.dot(synapse_1.T) * (layer_1 * (1-layer_1))\n",
    "    synapse_1 -= (alpha * layer_1.T.dot(layer_2_delta))\n",
    "    synapse_0 -= (alpha * X.T.dot(layer_1_delta))\n",
    "print \"Output After Training:\"\n",
    "print layer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
