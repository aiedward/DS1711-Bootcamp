{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recruit Restaurant Visitor Forecasting\n",
    "\n",
    "## Prerequisites\n",
    "Please make sure the following Python distributions and packages were installed.\n",
    "\n",
    "* [Anaconda](https://anaconda.org)\n",
    "* [XGBoost](https://github.com/dmlc/xgboost)\n",
    "* [LightGBM](https://github.com/Microsoft/LightGBM) - not needed by week 1\n",
    "* [Keras](https://keras.io) - not needed by week 1\n",
    "* [Tensorflow](https://www.tensorflow.org) - not needed by week 1\n",
    "* [Bayesian Optimization](https://github.com/fmfn/BayesianOptimization) - not needed by week 1\n",
    "* [seaborn](https://seaborn.pydata.org)\n",
    "* [bokeh](http://bokeh.pydata.org)\n",
    "\n",
    "You'll also need to create the following sub-folders in your working folder:\n",
    "\n",
    "* input\n",
    "   \n",
    "   To store all the data files downloaded from Kaggle\n",
    "   \n",
    "   \n",
    "* output\n",
    "    \n",
    "    To store submission files\n",
    "   \n",
    "   \n",
    "* python\n",
    "    \n",
    "    To store python scripts and ipython notebooks including this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "# import lightgbm as lgb - will be used in following weeks\n",
    "# from bayes_opt import BayesianOptimization - will be used in following weeks\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "from sklearn import preprocessing, pipeline, metrics, model_selection\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression,RidgeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../input/air_visit_data.csv')\n",
    "air_store_info = pd.read_csv('../input/air_store_info.csv')\n",
    "hpg_store_info = pd.read_csv('../input/hpg_store_info.csv')\n",
    "air_reserve = pd.read_csv('../input/air_reserve.csv')\n",
    "hpg_reserve = pd.read_csv('../input/hpg_reserve.csv')\n",
    "store_id_relation = pd.read_csv('../input/store_id_relation.csv')\n",
    "test_data = pd.read_csv('../input/sample_submission.csv')\n",
    "date_info = pd.read_csv('../input/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
    "train_size = train_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic preprocessing\n",
    "#### Take a look at train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_data.head())\n",
    "display(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split air_store_id and visit_date from id for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['visit_date'] = test_data['id'].map(lambda x: str(x).split('_')[2])\n",
    "test_data['air_store_id'] = test_data['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge training and testing data\n",
    "This is to simplify the efforts for feature engineering otherwise we'll have to perform the same transfromations for both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.concat([train_data,test_data])\n",
    "display(full_data.head())\n",
    "display(full_data.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datetime features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_data['visit_date'] = pd.to_datetime(full_data['visit_date'])\n",
    "full_data['dow'] = full_data['visit_date'].dt.dayofweek\n",
    "full_data['year'] = full_data['visit_date'].dt.year\n",
    "full_data['month'] = full_data['visit_date'].dt.month\n",
    "full_data['doy'] = full_data['visit_date'].dt.dayofyear\n",
    "full_data['dom'] = full_data['visit_date'].dt.days_in_month\n",
    "full_data['woy'] = full_data['visit_date'].dt.weekofyear\n",
    "full_data['is_month_end'] = full_data['visit_date'].dt.is_month_end\n",
    "full_data['visit_date'] = full_data['visit_date'].dt.date\n",
    "full_data['date_int'] = full_data['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store information\n",
    "\n",
    "There are two types of store information: air and hpg which are two websites where customers can make reservations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split area names\n",
    "\n",
    "It appears that the column area_name actually contains 3 levels of geographic area\n",
    "e.g Tōkyō-to Setagaya-ku Taishidō - > \n",
    "\n",
    "Tōkyō-to\n",
    "\n",
    "    Setagaya-ku\n",
    "\n",
    "        Taishidō\n",
    "Let's split it into 3 new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "air_store_info['air_area_lv1'] = air_store_info.air_area_name.apply(lambda x:x.split(' ')[0])\n",
    "air_store_info['air_area_lv2'] = air_store_info.air_area_name.apply(lambda x:x.split(' ')[1])\n",
    "air_store_info['air_area_lv3'] = air_store_info.air_area_name.apply(lambda x:x.split(' ')[2])\n",
    "\n",
    "hpg_store_info['hpg_area_lv1'] = hpg_store_info.hpg_area_name.apply(lambda x:x.split(' ')[0])\n",
    "hpg_store_info['hpg_area_lv2'] = hpg_store_info.hpg_area_name.apply(lambda x:x.split(' ')[1])\n",
    "hpg_store_info['hpg_area_lv3'] = hpg_store_info.hpg_area_name.apply(lambda x:x.split(' ')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_store_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create features that based on different levels of area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "air_store_info = pd.merge(air_store_info,\n",
    "                             air_store_info.groupby(['latitude','longitude']).air_store_id.count().\\\n",
    "                                    reset_index().rename(columns={'air_store_id':'air_stores_on_same_addr'}),\n",
    "                             how='left',\n",
    "                             on=['latitude','longitude'])\n",
    "\n",
    "\n",
    "air_store_info = pd.merge(air_store_info,\n",
    "                             air_store_info.groupby('air_area_lv1').air_store_id.count().\\\n",
    "                                    reset_index().rename(columns={'air_store_id':'air_stores_lv1'}),\n",
    "                             how='left',\n",
    "                             on='air_area_lv1')\n",
    "\n",
    "air_store_info = pd.merge(air_store_info,\n",
    "                             air_store_info.groupby(['air_area_lv1','air_area_lv2']).air_store_id.count().\\\n",
    "                                    reset_index().rename(columns={'air_store_id':'air_stores_lv2'}),\n",
    "                             how='left',\n",
    "                             on=['air_area_lv1','air_area_lv2'])\n",
    "\n",
    "\n",
    "air_store_info = pd.merge(air_store_info,\n",
    "                             air_store_info.groupby('air_area_lv1').latitude.mean().\\\n",
    "                                    reset_index().rename(columns={'latitude':'mean_lat_air_lv1'}),\n",
    "                             how='left',\n",
    "                             on='air_area_lv1')\n",
    "\n",
    "air_store_info = pd.merge(air_store_info,\n",
    "                             air_store_info.groupby('air_area_lv1').latitude.max().\\\n",
    "                                    reset_index().rename(columns={'latitude':'max_lat_air_lv1'}),\n",
    "                             how='left',\n",
    "                             on='air_area_lv1')\n",
    "\n",
    "air_store_info = pd.merge(air_store_info,\n",
    "                             air_store_info.groupby('air_area_lv1').latitude.min().\\\n",
    "                                    reset_index().rename(columns={'latitude':'min_lat_air_lv1'}),\n",
    "                             how='left',\n",
    "                             on='air_area_lv1')\n",
    "\n",
    "\n",
    "air_store_info = pd.merge(air_store_info,\n",
    "                             air_store_info.groupby('air_area_lv1').longitude.mean().\\\n",
    "                                    reset_index().rename(columns={'longitude':'mean_lon_air_lv1'}),\n",
    "                             how='left',\n",
    "                             on='air_area_lv1')\n",
    "\n",
    "air_store_info = pd.merge(air_store_info,\n",
    "                             air_store_info.groupby('air_area_lv1').longitude.max().\\\n",
    "                                    reset_index().rename(columns={'longitude':'max_lon_air_lv1'}),\n",
    "                             how='left',\n",
    "                             on='air_area_lv1')\n",
    "\n",
    "air_store_info = pd.merge(air_store_info,\n",
    "                             air_store_info.groupby('air_area_lv1').longitude.min().\\\n",
    "                                    reset_index().rename(columns={'longitude':'min_lon_air_lv1'}),\n",
    "                             how='left',\n",
    "                             on='air_area_lv1')\n",
    "\n",
    "air_store_info = pd.merge(air_store_info,\n",
    "                             air_store_info.groupby(['air_area_lv1','air_area_lv2']).latitude.mean().\\\n",
    "                                    reset_index().rename(columns={'latitude':'mean_lat_air_lv2'}),\n",
    "                             how='left',\n",
    "                             on=['air_area_lv1','air_area_lv2'])\n",
    "\n",
    "air_store_info = pd.merge(air_store_info,\n",
    "                             air_store_info.groupby(['air_area_lv1','air_area_lv2']).latitude.max().\\\n",
    "                                    reset_index().rename(columns={'latitude':'max_lat_air_lv2'}),\n",
    "                             how='left',\n",
    "                             on=['air_area_lv1','air_area_lv2'])\n",
    "\n",
    "air_store_info = pd.merge(air_store_info,\n",
    "                             air_store_info.groupby(['air_area_lv1','air_area_lv2']).latitude.min().\\\n",
    "                                    reset_index().rename(columns={'latitude':'min_lat_air_lv2'}),\n",
    "                             how='left',\n",
    "                             on=['air_area_lv1','air_area_lv2'])\n",
    "\n",
    "\n",
    "air_store_info = pd.merge(air_store_info,\n",
    "                             air_store_info.groupby(['air_area_lv1','air_area_lv2']).longitude.mean().\\\n",
    "                                    reset_index().rename(columns={'longitude':'mean_lon_air_lv2'}),\n",
    "                             how='left',\n",
    "                             on=['air_area_lv1','air_area_lv2'])\n",
    "\n",
    "air_store_info = pd.merge(air_store_info,\n",
    "                             air_store_info.groupby(['air_area_lv1','air_area_lv2']).longitude.max().\\\n",
    "                                    reset_index().rename(columns={'longitude':'max_lon_air_lv2'}),\n",
    "                             how='left',\n",
    "                             on=['air_area_lv1','air_area_lv2'])\n",
    "\n",
    "air_store_info = pd.merge(air_store_info,\n",
    "                             air_store_info.groupby(['air_area_lv1','air_area_lv2']).longitude.min().\\\n",
    "                                    reset_index().rename(columns={'longitude':'min_lon_air_lv2'}),\n",
    "                             how='left',\n",
    "                             on=['air_area_lv1','air_area_lv2'])\n",
    "\n",
    "\n",
    "hpg_store_info = pd.merge(hpg_store_info,\n",
    "                             hpg_store_info.groupby(['latitude','longitude']).hpg_store_id.count().\\\n",
    "                                    reset_index().rename(columns={'hpg_store_id':'hpg_stores_on_same_addr'}),\n",
    "                             how='left',\n",
    "                             on=['latitude','longitude'])\n",
    "\n",
    "\n",
    "hpg_store_info = pd.merge(hpg_store_info,\n",
    "                             hpg_store_info.groupby('hpg_area_lv1').hpg_store_id.count().\\\n",
    "                                    reset_index().rename(columns={'hpg_store_id':'hpg_stores_lv1'}),\n",
    "                             how='left',\n",
    "                             on='hpg_area_lv1')\n",
    "\n",
    "hpg_store_info = pd.merge(hpg_store_info,\n",
    "                             hpg_store_info.groupby(['hpg_area_lv1','hpg_area_lv2']).hpg_store_id.count().\\\n",
    "                                    reset_index().rename(columns={'hpg_store_id':'hpg_stores_lv2'}),\n",
    "                             how='left',\n",
    "                             on=['hpg_area_lv1','hpg_area_lv2'])\n",
    "\n",
    "\n",
    "hpg_store_info = pd.merge(hpg_store_info,\n",
    "                             hpg_store_info.groupby('hpg_area_lv1').latitude.mean().\\\n",
    "                                    reset_index().rename(columns={'latitude':'mean_lat_hpg_lv1'}),\n",
    "                             how='left',\n",
    "                             on='hpg_area_lv1')\n",
    "\n",
    "hpg_store_info = pd.merge(hpg_store_info,\n",
    "                             hpg_store_info.groupby('hpg_area_lv1').latitude.max().\\\n",
    "                                    reset_index().rename(columns={'latitude':'max_lat_hpg_lv1'}),\n",
    "                             how='left',\n",
    "                             on='hpg_area_lv1')\n",
    "\n",
    "hpg_store_info = pd.merge(hpg_store_info,\n",
    "                             hpg_store_info.groupby('hpg_area_lv1').latitude.min().\\\n",
    "                                    reset_index().rename(columns={'latitude':'min_lat_hpg_lv1'}),\n",
    "                             how='left',\n",
    "                             on='hpg_area_lv1')\n",
    "\n",
    "\n",
    "hpg_store_info = pd.merge(hpg_store_info,\n",
    "                             hpg_store_info.groupby('hpg_area_lv1').longitude.mean().\\\n",
    "                                    reset_index().rename(columns={'longitude':'mean_lon_hpg_lv1'}),\n",
    "                             how='left',\n",
    "                             on='hpg_area_lv1')\n",
    "\n",
    "hpg_store_info = pd.merge(hpg_store_info,\n",
    "                             hpg_store_info.groupby('hpg_area_lv1').longitude.max().\\\n",
    "                                    reset_index().rename(columns={'longitude':'max_lon_hpg_lv1'}),\n",
    "                             how='left',\n",
    "                             on='hpg_area_lv1')\n",
    "\n",
    "hpg_store_info = pd.merge(hpg_store_info,\n",
    "                             hpg_store_info.groupby('hpg_area_lv1').longitude.min().\\\n",
    "                                    reset_index().rename(columns={'longitude':'min_lon_hpg_lv1'}),\n",
    "                             how='left',\n",
    "                             on='hpg_area_lv1')\n",
    "\n",
    "hpg_store_info = pd.merge(hpg_store_info,\n",
    "                             hpg_store_info.groupby(['hpg_area_lv1','hpg_area_lv2']).latitude.mean().\\\n",
    "                                    reset_index().rename(columns={'latitude':'mean_lat_hpg_lv2'}),\n",
    "                             how='left',\n",
    "                             on=['hpg_area_lv1','hpg_area_lv2'])\n",
    "\n",
    "hpg_store_info = pd.merge(hpg_store_info,\n",
    "                             hpg_store_info.groupby(['hpg_area_lv1','hpg_area_lv2']).latitude.max().\\\n",
    "                                    reset_index().rename(columns={'latitude':'max_lat_hpg_lv2'}),\n",
    "                             how='left',\n",
    "                             on=['hpg_area_lv1','hpg_area_lv2'])\n",
    "\n",
    "hpg_store_info = pd.merge(hpg_store_info,\n",
    "                             hpg_store_info.groupby(['hpg_area_lv1','hpg_area_lv2']).latitude.min().\\\n",
    "                                    reset_index().rename(columns={'latitude':'min_lat_hpg_lv2'}),\n",
    "                             how='left',\n",
    "                             on=['hpg_area_lv1','hpg_area_lv2'])\n",
    "\n",
    "\n",
    "hpg_store_info = pd.merge(hpg_store_info,\n",
    "                             hpg_store_info.groupby(['hpg_area_lv1','hpg_area_lv2']).longitude.mean().\\\n",
    "                                    reset_index().rename(columns={'longitude':'mean_lon_hpg_lv2'}),\n",
    "                             how='left',\n",
    "                             on=['hpg_area_lv1','hpg_area_lv2'])\n",
    "\n",
    "hpg_store_info = pd.merge(hpg_store_info,\n",
    "                             hpg_store_info.groupby(['hpg_area_lv1','hpg_area_lv2']).longitude.max().\\\n",
    "                                    reset_index().rename(columns={'longitude':'max_lon_hpg_lv2'}),\n",
    "                             how='left',\n",
    "                             on=['hpg_area_lv1','hpg_area_lv2'])\n",
    "\n",
    "hpg_store_info = pd.merge(hpg_store_info,\n",
    "                             hpg_store_info.groupby(['hpg_area_lv1','hpg_area_lv2']).longitude.min().\\\n",
    "                                    reset_index().rename(columns={'longitude':'min_lon_hpg_lv2'}),\n",
    "                             how='left',\n",
    "                             on=['hpg_area_lv1','hpg_area_lv2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge store information for stores that exist in both air and hpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_store_info = pd.merge(air_store_info, store_id_relation, how='left', on='air_store_id')\n",
    "air_store_info = pd.merge(air_store_info, hpg_store_info, how='left', on='hpg_store_id')\n",
    "air_store_info = air_store_info.rename(columns={'latitude_x':'latitude_air',\n",
    "                             'longitude_x':'longitude_air',\n",
    "                             'latitude_y':'latitude_hpg',\n",
    "                             'longitude_y':'longitude_hpg'})\n",
    "\n",
    "display(air_store_info.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add store information to full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_data = pd.merge(full_data, air_store_info, how='left', on='air_store_id')\n",
    "display(full_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reservation information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('before')\n",
    "display(air_reserve.head())\n",
    "display(hpg_reserve.head())\n",
    "\n",
    "air_reserve['visit_date'] = air_reserve['visit_datetime'].apply(lambda x:x[:10])\n",
    "air_reserve['reserve_date'] = air_reserve['reserve_datetime'].apply(lambda x:x[:10])\n",
    "air_reserve['reserve_datetime'] = pd.to_datetime(air_reserve['reserve_datetime'])\n",
    "air_reserve['reserve_date'] = air_reserve['reserve_datetime'].dt.date\n",
    "air_reserve['visit_datetime'] = pd.to_datetime(air_reserve['visit_datetime'])\n",
    "air_reserve['visit_date'] = air_reserve['visit_datetime'].dt.date\n",
    "air_reserve['reserve_datetime_diff'] = air_reserve.apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).seconds \n",
    "                                                         * r['reserve_visitors']/3600/24.0, axis=1)\n",
    "\n",
    "\n",
    "hpg_reserve['visit_date'] = hpg_reserve['visit_datetime'].apply(lambda x:x[:10])\n",
    "hpg_reserve['reserve_date'] = hpg_reserve['reserve_datetime'].apply(lambda x:x[:10])\n",
    "hpg_reserve['reserve_datetime'] = pd.to_datetime(hpg_reserve['reserve_datetime'])\n",
    "hpg_reserve['reserve_date'] = hpg_reserve['reserve_datetime'].dt.date\n",
    "hpg_reserve['visit_datetime'] = pd.to_datetime(hpg_reserve['visit_datetime'])\n",
    "hpg_reserve['visit_date'] = hpg_reserve['visit_datetime'].dt.date\n",
    "hpg_reserve['reserve_datetime_diff'] = hpg_reserve.apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).seconds\n",
    "                                                         * r['reserve_visitors']/3600/24.0, axis=1)\n",
    "\n",
    "print ('after')\n",
    "display(air_reserve.head())\n",
    "display(hpg_reserve.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate reservations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_reserve_grp = air_reserve.groupby(['air_store_id','visit_date'])['reserve_visitors','reserve_datetime_diff'].\\\n",
    "            sum().reset_index().rename(columns={'reserve_visitors':'air_rvs',\n",
    "                                               'reserve_datetime_diff':'air_rv_dt_diff'})\n",
    "hpg_reserve_grp = hpg_reserve.groupby(['hpg_store_id','visit_date'])['reserve_visitors','reserve_datetime_diff'].\\\n",
    "            sum().reset_index().rename(columns={'reserve_visitors':'hpg_rvs',\n",
    "                                               'reserve_datetime_diff':'hpg_rv_dt_diff'})\n",
    "air_reserve_grp['mean_air_rv_dt_diff'] = air_reserve_grp.air_rv_dt_diff / air_reserve_grp.air_rvs\n",
    "hpg_reserve_grp['mean_hpg_rv_dt_diff'] = hpg_reserve_grp.hpg_rv_dt_diff / hpg_reserve_grp.hpg_rvs    \n",
    "    \n",
    "display(air_reserve_grp.head())\n",
    "display(hpg_reserve_grp.head())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add reservations to full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.merge(full_data, air_reserve_grp, how='left', on=['air_store_id','visit_date'])\n",
    "full_data = pd.merge(full_data, hpg_reserve_grp, how='left', on=['hpg_store_id','visit_date'])\n",
    "\n",
    "display(full_data.query('air_rvs>0 and hpg_rvs>0').head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date infomation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_info['visit_date'] = pd.to_datetime(date_info['visit_date'])\n",
    "date_info['dow'] = date_info['visit_date'].dt.dayofweek\n",
    "date_info['date_len'] = len(date_info)\n",
    "date_info['date_index'] = date_info.index + 1\n",
    "date_info['weight'] = ((date_info.index + 1) / len(date_info)) ** 5  \n",
    "date_info['visit_date'] = date_info['visit_date'].dt.date\n",
    "date_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.merge(full_data, date_info[['visit_date','date_len','date_index','weight', 'holiday_flg']], \n",
    "                     how='left', on='visit_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_vars = ['dow', 'year', 'month', 'doy', 'dom', 'woy', 'holiday_flg',\n",
    "            'date_index', 'date_int', 'air_stores_on_same_addr', 'hpg_stores_on_same_addr',\n",
    "            'latitude_air', 'longitude_air', \n",
    "            'air_stores_on_same_addr', 'air_stores_lv1', 'air_stores_lv2',\n",
    "            'mean_lat_air_lv1', 'max_lat_air_lv1', 'min_lat_air_lv1',\n",
    "            'mean_lon_air_lv1', 'max_lon_air_lv1', 'min_lon_air_lv1',\n",
    "            'mean_lat_air_lv2', 'max_lat_air_lv2', 'min_lat_air_lv2',\n",
    "            'mean_lon_air_lv2', 'max_lon_air_lv2', 'min_lon_air_lv2',\n",
    "            'latitude_hpg', 'longitude_hpg', \n",
    "            'hpg_stores_on_same_addr', 'hpg_stores_lv1', 'hpg_stores_lv2',\n",
    "            'mean_lat_hpg_lv1', 'max_lat_hpg_lv1', 'min_lat_hpg_lv1',\n",
    "            'mean_lon_hpg_lv1', 'max_lon_hpg_lv1', 'min_lon_hpg_lv1',\n",
    "            'mean_lat_hpg_lv2', 'max_lat_hpg_lv2', 'min_lat_hpg_lv2',\n",
    "            'mean_lon_hpg_lv2', 'max_lon_hpg_lv2', 'min_lon_hpg_lv2',\n",
    "            'air_rvs', 'hpg_rvs','air_rv_dt_diff', 'hpg_rv_dt_diff']\n",
    "\n",
    "\n",
    "cat_vars = ['air_store_id', 'air_genre_name', 'air_area_name', 'air_area_lv1', 'air_area_lv2', 'air_area_lv3',\n",
    "            'hpg_store_id', 'hpg_genre_name', 'hpg_area_name', 'hpg_area_lv1', 'hpg_area_lv2', 'hpg_area_lv3']\n",
    "\n",
    "id_var = 'air_store_id'\n",
    "target_var = 'visitors'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value imputation\n",
    "\n",
    "For this week we will leave missing values(Nan) as is and let XGBoost to take care of them automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Categorical features - label encoding\n",
    "\n",
    "Label encoding is not really necessary for this competition as all categorical features have already been digitalized. I'm including this just for your reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LBL = preprocessing.LabelEncoder()\n",
    "\n",
    "LE_vars=[]\n",
    "LE_map=dict()\n",
    "for cat_var in cat_vars:\n",
    "    print (\"Label Encoding %s\" % (cat_var))\n",
    "    LE_var=cat_var+'_le'\n",
    "    full_data[LE_var]=LBL.fit_transform(full_data[cat_var].astype(str))\n",
    "    LE_vars.append(LE_var)\n",
    "    LE_map[cat_var]=LBL.classes_\n",
    "    \n",
    "print (\"Label-encoded feaures: %s\" % (LE_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features - one hot encoding¶\n",
    "\n",
    "You don't want to concatenate the converted OHE features with the original dataframe(full_data) becuase it would exponentially enlarge the size of the dataframe. In fact, it's recommended to use scipy.sparse.hstack to concatenate the data which you will see in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OHE = preprocessing.OneHotEncoder(sparse=True)\n",
    "start=time.time()\n",
    "OHE.fit(full_data[LE_vars])\n",
    "OHE_sparse=OHE.transform(full_data[LE_vars])\n",
    "                                   \n",
    "print ('One-hot-encoding finished in %f seconds' % (time.time()-start))\n",
    "\n",
    "\n",
    "OHE_vars = [var[:-3] + '_' + str(level).replace(' ','_')\\\n",
    "                for var in cat_vars for level in LE_map[var] ]\n",
    "\n",
    "print (\"OHE_sparse size :\" ,OHE_sparse.shape)\n",
    "print (\"One-hot encoded catgorical feature samples : %s\" % (OHE_vars[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric features\n",
    "\n",
    "For week 1 and week 2 we will be using XGBoost/LightGBM which typically don't require pre-processing for numeric features so we will skip this part until week 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature interactions\n",
    "### numeric to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['total_rvs'] = full_data['air_rvs'] + full_data['hpg_rvs']\n",
    "full_data['mean_rvs_air_hpg'] = full_data[['air_rvs','hpg_rvs']].apply(lambda x:np.mean(x), axis=1)\n",
    "full_data['mean_dt_diff_air_hpg'] = full_data[['air_rv_dt_diff','hpg_rv_dt_diff']].apply(lambda x:np.mean(x), axis=1)\n",
    "\n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "full_data['lon_plus_lat_air'] = full_data['longitude_air'] + full_data['latitude_air'] \n",
    "\n",
    "full_data['lat_to_mean_lat_air_lv1'] = abs(full_data['latitude_air']-full_data['mean_lat_air_lv1'])\n",
    "full_data['lat_to_max_lat_air_lv1']  = full_data['latitude_air']-full_data['max_lat_air_lv1']\n",
    "full_data['lat_to_min_lat_air_lv1']  = full_data['latitude_air']-full_data['min_lat_air_lv1']\n",
    "full_data['lon_to_mean_lon_air_lv1']  = abs(full_data['longitude_air']-full_data['mean_lon_air_lv1'])\n",
    "full_data['lon_to_max_lon_air_lv1']  = full_data['longitude_air']-full_data['max_lon_air_lv1']\n",
    "full_data['lon_to_min_lon_air_lv1']  = full_data['longitude_air']-full_data['min_lon_air_lv1']\n",
    "full_data['lat_to_mean_lat_air_lv2'] = abs(full_data['latitude_air']-full_data['mean_lat_air_lv2'])\n",
    "full_data['lat_to_max_lat_air_lv2']  = full_data['latitude_air']-full_data['max_lat_air_lv2']\n",
    "full_data['lat_to_min_lat_air_lv2']  = full_data['latitude_air']-full_data['min_lat_air_lv2']\n",
    "full_data['lon_to_mean_lon_air_lv2'] = abs(full_data['longitude_air']-full_data['mean_lon_air_lv2'])\n",
    "full_data['lon_to_max_lon_air_lv2']  = full_data['longitude_air']-full_data['max_lon_air_lv2']\n",
    "full_data['lon_to_min_lon_air_lv2']  = full_data['longitude_air']-full_data['min_lon_air_lv2']\n",
    "\n",
    "full_data['lat_to_mean_lat_hpg_lv1'] = abs(full_data['latitude_hpg']-full_data['mean_lat_hpg_lv1'])\n",
    "full_data['lat_to_max_lat_hpg_lv1']  = full_data['latitude_hpg']-full_data['max_lat_hpg_lv1']\n",
    "full_data['lat_to_min_lat_hpg_lv1']  = full_data['latitude_hpg']-full_data['min_lat_hpg_lv1']\n",
    "full_data['lon_to_mean_lon_hpg_lv1']  = abs(full_data['longitude_hpg']-full_data['mean_lon_hpg_lv1'])\n",
    "full_data['lon_to_max_lon_hpg_lv1']  = full_data['longitude_hpg']-full_data['max_lon_hpg_lv1']\n",
    "full_data['lon_to_min_lon_hpg_lv1']  = full_data['longitude_hpg']-full_data['min_lon_hpg_lv1']\n",
    "full_data['lat_to_mean_lat_hpg_lv2'] = abs(full_data['latitude_hpg']-full_data['mean_lat_hpg_lv2'])\n",
    "full_data['lat_to_max_lat_hpg_lv2']  = full_data['latitude_hpg']-full_data['max_lat_hpg_lv2']\n",
    "full_data['lat_to_min_lat_hpg_lv2']  = full_data['latitude_hpg']-full_data['min_lat_hpg_lv2']\n",
    "full_data['lon_to_mean_lon_hpg_lv2'] = abs(full_data['longitude_hpg']-full_data['mean_lon_hpg_lv2'])\n",
    "full_data['lon_to_max_lon_hpg_lv2']  = full_data['longitude_hpg']-full_data['max_lon_hpg_lv2']\n",
    "full_data['lon_to_min_lon_hpg_lv2']  = full_data['longitude_hpg']-full_data['min_lon_hpg_lv2']\n",
    "\n",
    "num_num_vars = ['total_rvs', 'mean_rvs_air_hpg',\n",
    "       'mean_dt_diff_air_hpg', 'lon_plus_lat_air',\n",
    "       'lat_to_mean_lat_air_lv1', 'lat_to_max_lat_air_lv1',\n",
    "       'lat_to_min_lat_air_lv1', 'lon_to_mean_lon_air_lv1',\n",
    "       'lon_to_max_lon_air_lv1', 'lon_to_min_lon_air_lv1',\n",
    "       'lat_to_mean_lat_air_lv2', 'lat_to_max_lat_air_lv2',\n",
    "       'lat_to_min_lat_air_lv2', 'lon_to_mean_lon_air_lv2',\n",
    "       'lon_to_max_lon_air_lv2', 'lon_to_min_lon_air_lv2',\n",
    "       'lat_to_mean_lat_hpg_lv1', 'lat_to_max_lat_hpg_lv1',\n",
    "       'lat_to_min_lat_hpg_lv1', 'lon_to_mean_lon_hpg_lv1',\n",
    "       'lon_to_max_lon_hpg_lv1', 'lon_to_min_lon_hpg_lv1',\n",
    "       'lat_to_mean_lat_hpg_lv2', 'lat_to_max_lat_hpg_lv2',\n",
    "       'lat_to_min_lat_hpg_lv2', 'lon_to_mean_lon_hpg_lv2',\n",
    "       'lon_to_max_lon_hpg_lv2', 'lon_to_min_lon_hpg_lv2']\n",
    "\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### categorical to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['air_area_genre'] = full_data['air_area_name'] + '-' + full_data['air_genre_name'] \n",
    "full_data['air_store_dow'] = full_data['air_store_id'] + '-' + full_data['dow'].astype(str)\n",
    "full_data['air_store_dow_holiday'] = full_data['air_store_id'] + '-' + full_data['dow'].astype(str) + '-' + full_data['holiday_flg'].astype(str)\n",
    "\n",
    "cat_cat_vars = ['air_area_genre','air_store_dow', 'air_store_dow_holiday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LBL = preprocessing.LabelEncoder()\n",
    "\n",
    "cat_cat_LE_vars=[]\n",
    "cat_cat_LE_map=dict()\n",
    "for cat_var in cat_cat_vars:\n",
    "    print (\"Label Encoding %s\" % (cat_var))\n",
    "    LE_var=cat_var+'_le'\n",
    "    full_data[LE_var]=LBL.fit_transform(full_data[cat_var].astype(str))\n",
    "    cat_cat_LE_vars.append(LE_var)\n",
    "    cat_cat_LE_map[cat_var]=LBL.classes_\n",
    "    \n",
    "print (\"Label-encoded feaures: %s\" % (cat_cat_LE_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target aggregation\n",
    "\n",
    "This sometimes works for timeseries problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = full_data[:train_size].groupby(['air_store_id','dow','holiday_flg'])['visitors'].\\\n",
    "            agg([np.mean, np.max, np.min, np.median]).\\\n",
    "            reset_index().\\\n",
    "            rename(columns={'mean':'mean_visitors',\n",
    "                           'amax':'max_visitors',\n",
    "                           'amin':'min_visitors',\n",
    "                           'median':'median_visitors'})\n",
    "            \n",
    "full_data = pd.merge(full_data, tmp, how='left', on=['air_store_id','dow','holiday_flg'])\n",
    "\n",
    "tmp = full_data[:train_size].groupby(['air_store_id','dow', 'holiday_flg']).\\\n",
    "            apply(lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )).\\\n",
    "            reset_index().rename(columns={0:'wmean_visitors'})\n",
    "        \n",
    "full_data = pd.merge(full_data, tmp, how='left', on=['air_store_id','dow','holiday_flg'])    \n",
    "target_aggr_vars = ['mean_visitors', 'max_visitors', 'min_visitors', 'median_visitors', 'wmean_visitors']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "We all run XGBoost models using a couple of combinations of features as well as with different missing value settings just to see how differently they perform.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numerical features + label-encoded categorical features\n",
    "Let's get started with the simplest combination: **numerical features + label-encoded categorical features** without additional transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_vars = num_vars + LE_vars + num_num_vars + cat_cat_LE_vars + target_aggr_vars\n",
    "    \n",
    "train = full_data[:train_size]\n",
    "y = full_data[:train_size][target_var][:train_size].values\n",
    "test = full_data[train_size:]\n",
    "ids = full_data[train_size:][train_size:].id.values\n",
    "\n",
    "print ('train data size:', train.shape, 'test data size:', test.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train[(train['visit_date']<=datetime.datetime.strptime('2017-03-09', '%Y-%m-%d').date()) \n",
    "      & (train['visit_date']>datetime.datetime.strptime('2016-04-01', '%Y-%m-%d').date())][full_vars].values\n",
    "train_y = np.log1p(train[(train['visit_date']<=datetime.datetime.strptime('2017-03-09', '%Y-%m-%d').date()) \n",
    "      & (train['visit_date']>datetime.datetime.strptime('2016-04-01', '%Y-%m-%d').date())]['visitors'].values)\n",
    "\n",
    "val_x = train[(train['visit_date']>datetime.datetime.strptime('2017-03-09', '%Y-%m-%d').date())][full_vars].values\n",
    "val_y = np.log1p(train[(train['visit_date']>datetime.datetime.strptime('2017-03-09', '%Y-%m-%d').date())]['visitors'].values)\n",
    "print (train_x.shape, val_x.shape, train_x.shape[0]+ val_x.shape[0])\n",
    "\n",
    "\n",
    "xgtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "xgval=xgb.DMatrix(val_x,label=val_y)\n",
    "\n",
    "watchlist  = [ (xgtrain,'train'),(xgval,'eval')]\n",
    "\n",
    "\n",
    "best_xgb_params = {'colsample_bytree': 0.7,\n",
    " 'eta': 0.1,\n",
    " 'gamma': 1,\n",
    " 'max_depth': 10,\n",
    " 'min_child_weight': 3,\n",
    " 'nthread': 8,\n",
    " 'objective': 'reg:linear',\n",
    " 'seed': 1234,\n",
    " 'subsample': 1}\n",
    "\n",
    "print (best_xgb_params)\n",
    "\n",
    "model = xgb.train(best_xgb_params, \n",
    "                  xgtrain, \n",
    "                  num_boost_round=100000,\n",
    "                  evals=watchlist,\n",
    "                  early_stopping_rounds=50,\n",
    "                  verbose_eval=50)    \n",
    "best_iteration = model.best_iteration\n",
    "best_score = model.best_score\n",
    "print ('best_score: %f, best_iteration: %d' % (best_score, best_iteration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_names = full_vars\n",
    "feature_importance = pd.DataFrame.from_dict(model.get_fscore(), orient='index')\n",
    "feature_importance.columns = ['importance']\n",
    "feature_importance.importance = feature_importance.importance/ feature_importance.importance.sum()\n",
    "feature_importance.sort_values(by='importance').head(30).plot(kind='barh',figsize=(4,20))\n",
    "feature_importance.sort_values(by='importance',ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train[full_vars].values\n",
    "train_y = np.log1p(train['visitors'].values)\n",
    "model = xgb.train(best_xgb_params, \n",
    "                  xgb.DMatrix(train_x, label=train_y), \n",
    "                  num_boost_round=best_iteration)    \n",
    "test['visitors'] = model.predict(xgb.DMatrix(test[full_vars].values))\n",
    "test['visitors'] = np.expm1(test['visitors']).clip(lower=0.)\n",
    "sub = test[['id','visitors']].copy()\n",
    "sub[['id', 'visitors']].to_csv('../output/sub_starter.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Feature engineering is the key, if not the most important, to the success of a data science projecct including Kaggle competition. It requires a data scientist to have excellent knowledge of Machine Learning algorithms, good sense of business, programming skills and, last but not least, hacker spirits.\n",
    "\n",
    "In this week's lecture we've learnt how to:\n",
    "\n",
    "* Preprocess data for\n",
    "    * Numeric features\n",
    "    * Categorical features\n",
    "* Impute missing values\n",
    "* Select features\n",
    "\n",
    "# Recommended Kaggle posts:\n",
    "\n",
    "* [A Very Extensive Recruit Exploratory Analysis](https://www.kaggle.com/captcalculator/a-very-extensive-recruit-exploratory-analysis)\n",
    "* [https://www.kaggle.com/headsortails/be-my-guest-recruit-restaurant-eda](https://www.kaggle.com/headsortails/be-my-guest-recruit-restaurant-eda)\n",
    "* [Surprise Me](https://www.kaggle.com/the1owl/surprise-me)\n",
    "* [Things that make this competition interesting (and fun)](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/45120)\n",
    "\n",
    "\n",
    "# Additional readings\n",
    "* [Applied Predictive Modeling - Chapter 3 Data Pre-Processing](http://appliedpredictivemodeling.com/toc/)\n",
    "* [机器学习特征工程实用技巧大全](https://zhuanlan.zhihu.com/p/26444240)\n",
    "* [Discover Feature Engineering](http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)\n",
    "* [Selecting good features – Part IV: stability selection, RFE and everything side by side](http://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/)\n",
    "\n",
    "\n",
    "# Assignments\n",
    "1. Run this notebook and make submissions\n",
    "2. Experiment whatever feature engineering you could think of and see how they perform"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
